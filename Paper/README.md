# Neural_Accleration_Lab_Paper_List

본 Lab에서 발표하는 논문 리스트는 다음과 같습니다.
11월 8일부터 진행된 본 랩에서는 참여 인원당 최소 2개 이상의 논문을 발표하였습니다.
발표에 사용된 자료와 논문 리스트를 해당 Git은 포함하고 있습니다.

|  <center>번호</center> |  <center>논문명</center> | <center>학회</center> | <center>분야</center> | <center>출판일</center> |
|:--------:|:--------:|:--------:|:--------:|:--------:|
|<center>1</center> | <center>Scalpel: Customizing DNN Pruning to the Underlying Hardware Parallelism</center> | <center>ISCA</center> | <center>Pruning</center>|<center>2017</center>|
|<center>2</center> | <center>Diagonalwise Refactorization: An Efficient Training Method for Depthwise Convolutions</center> |IJCNN | Algorithm | 2018 |
|<center>3</center> | Optimal DNN Primitive Selection with Partitioned Boolean quadratic Programming |CGO | Primitive Selection | 2018 |
|<center>4</center> | Co-Design of Deep Neural Nets and Neural Net Accelerators for Embedded Vision Applications |DAC | NPU | 2018 |
|<center>5</center> | µLayer:Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization |EuroSys | CPU+GPU | 2019 |
|<center>6</center> | Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge |ASPLOS | Edge+Server | 2017 |
|<center>7</center> | Efficient Mobile Implementation of a CNN-based Object Recognitino System |ACM Multimedia | BLAS Design| 2019 |
|<center>8</center> | Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs |Micro | Tensor Core | 2019 |
|<center>9</center> | EFFICIENT WINOGRAD CONVOLUTION VIA INTEGER ARITHMETIC |Arxiv | Algorithm | 2019 |
|<center>10</center> | Optimizing CNN Model Inference on CPUs |Usenix | ? | 2019 |
|<center>11</center> | Machine Learning at Facebook: Understanding Inference at the Edge |HPCA | Overview Paper | 2019 |
|<center>12</center> | Rethinking floating point for deep learning |NIPS | Algorithm | 2018 |
|<center>13</center> | Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications |Arxiv | Overview Paper | 2018 |
|<center>14</center> | EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks |CVPR | Neural Network | 2019 |
|<center>15</center> | Data-Free Quantization Through Weight Equalization and Bias Correction |ICCV | Quantization | 2019 |
|<center>16</center> | The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks|ICLR | Pruning | 2019 |
|<center>17</center> | Neural Network Inference on Mobile SoCs| Arxiv | Acceleration | 2019 |





