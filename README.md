# Neural_Accleration_Lab
모두의 연구소에서 진행하는 Neural Acceleration Lab의 git 입니다. 

본 Lab에서 발표하는 논문 리스트는 다음과 같습니다.

|  <center>번호</center> |  <center>논문명</center> | <center>학회</center> | <center>분야</center> | <center>출판일</center> |
|:--------:|:--------:|:--------:|:--------:|:--------:|
|<center>1</center> | <center>Scalpel: Customizing DNN Pruning to the Underlying Hardware Parallelism</center> | <center>ISCA</center> | <center>Pruning</center>|<center>2017</center>|
|<center>2</center> | <center>Diagonalwise Refactorization: An Efficient Training Method for Depthwise Convolutions</center> |IJCNN | Algorithm | 2018 |
|<center>3</center> | Optimal DNN Primitive Selection with Partitioned Boolean quadratic Programming |IJCNN | Algorithm | 2018 |
|<center>4</center> | Co-Design of Deep Neural Nets and Neural Net Accelerators for Embedded Vision Applications |IJCNN | Algorithm | 2018 |
|<center>5</center> | µLayer:Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization |IJCNN | Algorithm | 2018 |
|<center>6</center> | Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge |IJCNN | Algorithm | 2018 |
|<center>7</center> | TSM2: Optimizing Tall-and-Skinny Matrix-Matrix Multiplication on GPUs |IJCNN | Algorithm | 2018 |
|<center>8</center> | Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs |IJCNN | Algorithm | 2018 |
|<center>9</center> | EFFICIENT WINOGRAD CONVOLUTION VIA INTEGER ARITHMETIC |IJCNN | Algorithm | 2018 |
|<center>10</center> | <center>Diagonalwise Refactorization: An Efficient Training Method for Depthwise Convolutions</center> |IJCNN | Algorithm | 2018 |
|<center>11</center> | <center>Diagonalwise Refactorization: An Efficient Training Method for Depthwise Convolutions</center> |IJCNN | Algorithm | 2018 |



